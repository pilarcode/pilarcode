


# ðŸ§ª Agents and LLM Observability 

Great evals start with great observability.

 ## What is observability?
 Observability is the ability to understand the internal state of a system by examining its outputs. In the context of software, this means being able to understand the internal state of a system by examining its telemetry data, which includes traces, metrics, and logs.

## Observability frameworks

- [opentelemetry](https://opentelemetry.io/) An observability framework and toolkit designed to facilitate the Generation Export Collection of telemetry data such as traces, metrics, and logs.
- [Traceloop](https://github.com/traceloop/openllmetry) Open-source observability for your LLM application, based on OpenTelemetry
- [AgentOps](https://www.agentops.ai/) Python SDK for AI agent monitoring, LLM cost tracking, benchmarking, and more. Integrates with most LLMs and agent frameworks including CrewAI, Agno, OpenAI Agents SDK, Langchain, Autogen, AG2, and CamelAI
- [Pydantic logfire](https://pydantic.dev/logfire) Visualise your whole app, not just LLM calls
- [Arize](https://arize.com/)
- [Langsmith](https://smith.langchain.com/)
- [LangFuse](https://langfuse.com/)
- [Helicone](https://www.helicone.ai/)
- [PromptFlow](https://microsoft.github.io/promptflow/tutorials/trace-llm.html)
- [MLflow](https://mlflow.org/docs/latest/tracing/)
- [Comet](https://www.comet.com)
- [Galileo](https://www.galileo.ai/)
- [Signoz](https://signoz.io/) Get APM, logs, traces, metrics, exceptions, & alerts in a single tool

## Tracing
- [ Jaeger Tracing Platform](https://www.jaegertracing.io/)


# ðŸ§ª Evals
- [OpenAI Evals](https://cookbook.openai.com/topic/evals)
- [Weave](https://wandb.ai/site/weave/) Weave is a toolkit built by Weights & Biases for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications.
- [Locust](https://github.com/locustio/locust): Locust's AI load tests can not only call your LLM, but also bypass it and call tools directly.
- [Epoch](https://epoch.ai/): Investigating the trajectory of AI
- [Confident AI](https://deepeval.com/) Confident AI is the cloud platform for DeepEval, the most widely adopted open-source framework to evaluate LLM applications such as RAG pipielines, agentics, chatbots, or even just an LLM itself.
- [langwatch](https://langwatch.ai/) Test your AI agents with simulated users
- [Laminar](https://www.lmnr.ai/) Laminar is the open-source platform for tracing and evaluating AI applications
- [Patronus ](https://www.patronus.ai/)
- [Percival](https://www.patronus.ai/percival)
