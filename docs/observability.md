
# ðŸ§ªObservability:  Great evals start with great observability.
Observability is the ability to understand the internal state of a system by examining its outputs. In the context of software, this means being able to understand the internal state of a system by examining its telemetry data, which includes traces, metrics, and logs.

## Observability frameworks
- [Traceloop](https://github.com/traceloop/openllmetry) Open-source observability for your LLM application, based on OpenTelemetry
- [Phoenix](https://phoenix.arize.com/) Open-source LLM tracing and evaluation
- [AgentOps](https://www.agentops.ai/) Trace, Debug, & Deploy Reliable AI Agents.
- [Pydantic logfire](https://pydantic.dev/logfire) Visualise your whole app, not just LLM calls
- [Langsmith](https://smith.langchain.com/)
- [LangFuse](https://langfuse.com/)
- [Helicone](https://www.helicone.ai/)
- [PromptFlow](https://microsoft.github.io/promptflow/tutorials/trace-llm.html)
- [MLflow](https://mlflow.org/docs/latest/tracing/)
- [Comet](https://www.comet.com)
- [Galileo](https://www.galileo.ai/)
- [Signoz](https://signoz.io/) Get APM, logs, traces, metrics, exceptions, & alerts in a single tool
- [Jaeger Tracing Platform](https://www.jaegertracing.io/)


