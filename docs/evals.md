
# ðŸ§ª Evals
- [OpenAI Evals](https://cookbook.openai.com/topic/evals)
- [Weave](https://wandb.ai/site/weave/) Weave is a toolkit built by Weights & Biases for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications.
- [Locust](https://github.com/locustio/locust): Locust's AI load tests can not only call your LLM, but also bypass it and call tools directly.
- [Epoch](https://epoch.ai/): Investigating the trajectory of AI
- [Confident AI](https://deepeval.com/) Confident AI is the cloud platform for DeepEval, the most widely adopted open-source framework to evaluate LLM applications such as RAG pipielines, agentics, chatbots, or even just an LLM itself.
- [langwatch](https://langwatch.ai/) Test your AI agents with simulated users
- [Laminar](https://www.lmnr.ai/) Laminar is the open-source platform for tracing and evaluating AI applications
- [Patronus ](https://www.patronus.ai/)
- [Percival](https://www.patronus.ai/percival)
